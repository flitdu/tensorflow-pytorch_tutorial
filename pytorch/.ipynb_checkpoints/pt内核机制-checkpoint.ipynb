{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pt 优点：\n",
    "简洁易懂：Pytorch的API设计的相当简洁一致。基本上就是tensor, autograd, nn三级封装。学习起来非常容易。\n",
    "Pytorch底层最核心的概念是张量，动态计算图以及自动微分。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 张量\n",
    "- dtype\n",
    "- shape/ size()\n",
    "- dim()  维度：0，1，2，3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) torch.int64\n",
      "tensor(2.) torch.float32\n",
      "tensor(True) torch.bool\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "# 自动推断数据类型\n",
    "\n",
    "i = torch.tensor(1);print(i,i.dtype)\n",
    "x = torch.tensor(2.0);print(x,x.dtype)\n",
    "b = torch.tensor(True);print(b,b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "0\n",
      "torch.bool\n",
      "torch.float32\n",
      "SIZE:  torch.Size([])\n",
      "SIZE:  torch.Size([4])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(True)\n",
    "print(scalar);print(scalar.dim());print(scalar.dtype)\n",
    "a = torch.tensor(2.0)\n",
    "print(a.dtype)\n",
    "print('SIZE: ', scalar.size())\n",
    "vector = torch.tensor([1.0,2.0,3.0,4.0])\n",
    "print('SIZE: ', vector.size())\n",
    "print(vector.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 1., 1.],\n",
      "         [1., 3., 4., 1.]],\n",
      "\n",
      "        [[2., 5., 6., 1.],\n",
      "         [0., 7., 8., 1.]],\n",
      "\n",
      "        [[1., 5., 6., 1.],\n",
      "         [0., 7., 8., 1.]]])\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 4]), torch.Size([3, 2, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor3 = torch.tensor([[[1.0,2.0,1,1],[1,3.0,4.0,1]],\n",
    "                        [[2,5.0,6.0,1],[0, 7.0,8.0,1]],\n",
    "                        [[1,5.0,6.0,1],[0, 7.0,8.0,1]]])  # 3维张量\n",
    "print(tensor3)\n",
    "print(tensor3.dim())\n",
    "tensor3.shape, tensor3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0578, 0.4039, 0.7349],\n",
      "        [0.8830, 0.5480, 0.0786]])\n",
      "tensor([ True, False,  True, False]) torch.bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " tensor([0.8830, 0.5480, 0.0786]),\n",
       " tensor(0.5480),\n",
       " tensor([0.0578, 0.8830]),\n",
       " tensor([0.7349, 0.0786]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量切片\n",
    "import numpy as np\n",
    "\n",
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "b = torch.BoolTensor(np.array([1,0,2,0])); print(b,b.dtype)\n",
    "a.dim(), a[1,:], a[1,1], a[:,0], a[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 动态计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pt 使用动态计算图\n",
    "> PyTorch 使用一种称之为 imperative / eager 的范式，即每一行代码都要求构建一个图以定义完整计算图的一个部分。即使完整的计算图还没有完成构建，我们也可以独立地执行这些作为组件的小计算图，这种动态计算图被称为「define-by-run」方法。\n",
    "\n",
    "![image.png](https://image.jiqizhixin.com/uploads/editor/b6aea2ac-a4b4-4314-8b18-c1b4a1a79ca4/640.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 backward()方法求导\n",
    "通常用在标量张量上，该方法求得的梯度将存在对应自变量张量的grad属性下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# 标量的反向传播\n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c的导数\n",
    "# >>2ax+b\n",
    "\n",
    "x = torch.tensor(0.0,requires_grad = True) # x需要被求导\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "y = a*torch.pow(x,2) + b*x + c \n",
    "\n",
    "y.backward()\n",
    "dy_dx = x.grad\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
