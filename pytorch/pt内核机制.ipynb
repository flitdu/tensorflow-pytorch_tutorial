{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pt 优点：\n",
    "简洁易懂：Pytorch的API设计的相当简洁一致。基本上就是tensor, autograd, nn三级封装。学习起来非常容易。\n",
    "Pytorch底层最核心的概念是张量，动态计算图以及自动微分。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 张量\n",
    "- dtype\n",
    "- shape/ size()\n",
    "- dim()  维度：0，1，2，3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) torch.int64\n",
      "tensor(2.) torch.float32\n",
      "tensor(True) torch.bool\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "# 自动推断数据类型\n",
    "\n",
    "i = torch.tensor(1);print(i,i.dtype)\n",
    "x = torch.tensor(2.0);print(x,x.dtype)\n",
    "b = torch.tensor(True);print(b,b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "0\n",
      "torch.bool\n",
      "torch.float32\n",
      "SIZE:  torch.Size([])\n",
      "SIZE:  torch.Size([4])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(True)\n",
    "print(scalar);print(scalar.dim());print(scalar.dtype)\n",
    "a = torch.tensor(2.0)\n",
    "print(a.dtype)\n",
    "print('SIZE: ', scalar.size())\n",
    "vector = torch.tensor([1.0,2.0,3.0,4.0])\n",
    "print('SIZE: ', vector.size())\n",
    "print(vector.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 1., 1.],\n",
      "         [1., 3., 4., 1.]],\n",
      "\n",
      "        [[2., 5., 6., 1.],\n",
      "         [0., 7., 8., 1.]],\n",
      "\n",
      "        [[1., 5., 6., 1.],\n",
      "         [0., 7., 8., 1.]]])\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 4]), torch.Size([3, 2, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor3 = torch.tensor([[[1.0,2.0,1,1],[1,3.0,4.0,1]],\n",
    "                        [[2,5.0,6.0,1],[0, 7.0,8.0,1]],\n",
    "                        [[1,5.0,6.0,1],[0, 7.0,8.0,1]]])  # 3维张量\n",
    "print(tensor3)\n",
    "print(tensor3.dim())\n",
    "tensor3.shape, tensor3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0578, 0.4039, 0.7349],\n",
      "        [0.8830, 0.5480, 0.0786]])\n",
      "tensor([ True, False,  True, False]) torch.bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " tensor([0.8830, 0.5480, 0.0786]),\n",
       " tensor(0.5480),\n",
       " tensor([0.0578, 0.8830]),\n",
       " tensor([0.7349, 0.0786]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量切片\n",
    "import numpy as np\n",
    "\n",
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "b = torch.BoolTensor(np.array([1,0,2,0])); print(b,b.dtype)\n",
    "a.dim(), a[1,:], a[1,1], a[:,0], a[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 动态计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pt 使用动态计算图\n",
    "> PyTorch 使用一种称之为 imperative / eager 的范式，即每一行代码都要求构建一个图以定义完整计算图的一个部分。即使完整的计算图还没有完成构建，我们也可以独立地执行这些作为组件的小计算图，这种动态计算图被称为「define-by-run」方法。\n",
    "\n",
    "![image.png](https://image.jiqizhixin.com/uploads/editor/b6aea2ac-a4b4-4314-8b18-c1b4a1a79ca4/640.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算图TensorBoard中可视化\n",
    "\n",
    "![image.png](../pic/pt-board.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算图可视化\n",
    "from torch import nn \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = nn.Parameter(torch.randn(2,1))\n",
    "        self.b = nn.Parameter(torch.zeros(1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x@self.w + self.b\n",
    "        return y\n",
    "\n",
    "net = Net()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('../../model/tensorboard')\n",
    "writer.add_graph(net,input_to_model = torch.rand(10,2))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir ../../model/tensorboard (started 0:27:36 ago; pid 17349)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17349), started 0:27:36 ago. (Use '!kill 17349' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ee70a7d5e0d34c6a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ee70a7d5e0d34c6a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%tensorboard --logdir ../../model/tensorboard\n",
    "\n",
    "from tensorboard import notebook\n",
    "notebook.list() \n",
    "\n",
    "#在tensorboard中查看模型\n",
    "notebook.start(\"--logdir ../../model/tensorboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 backward()方法求导\n",
    "通常用在标量张量上，该方法求得的梯度将存在对应自变量张量的grad属性下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# 标量的反向传播\n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c的导数\n",
    "# >>2ax+b\n",
    "\n",
    "x = torch.tensor(0.0,requires_grad = True) # x需要被求导\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "y = a*torch.pow(x,2) + b*x + c \n",
    "\n",
    "y.backward()\n",
    "dy_dx = x.grad\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[0., 0.],\n",
      "        [1., 2.]], requires_grad=True)\n",
      "y:\n",
      " tensor([[1., 1.],\n",
      "        [0., 1.]], grad_fn=<AddBackward0>)\n",
      "x_grad:\n",
      " tensor([[-4., -2.],\n",
      "        [ 0.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "# 非标量的反向传播\n",
    "# 如果调用的张量非标量，则要传入一个和它同形状 的gradient参数张量。\n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c\n",
    "\n",
    "x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x需要被求导\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "y = a*torch.pow(x,2) + b*x + c \n",
    "\n",
    "gradient = torch.tensor([[2.0,1.0],[1.0,1.0]])  # <---传递，其中gradient 表示权重\n",
    "\n",
    "print(\"x:\\n\",x)\n",
    "print(\"y:\\n\",y)\n",
    "y.backward(gradient = gradient)\n",
    "x_grad = x.grad\n",
    "print(\"x_grad:\\n\",x_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n",
      "tensor(-1.9600)\n",
      "tensor(-1.9208)\n",
      "tensor(-1.8824)\n",
      "tensor(-1.8447)\n",
      "tensor(-1.8078)\n",
      "tensor(-1.7717)\n",
      "tensor(-1.7363)\n",
      "tensor(-1.7015)\n",
      "tensor(-1.6675)\n",
      "tensor(-1.6341)\n",
      "tensor(-1.6015)\n",
      "tensor(-1.5694)\n",
      "tensor(-1.5380)\n",
      "tensor(-1.5073)\n",
      "tensor(-1.4771)\n",
      "tensor(-1.4476)\n",
      "tensor(-1.4186)\n",
      "tensor(-1.3903)\n",
      "tensor(-1.3625)\n",
      "tensor(-1.3352)\n",
      "tensor(-1.3085)\n",
      "tensor(-1.2823)\n",
      "tensor(-1.2567)\n",
      "tensor(-1.2316)\n",
      "tensor(-1.2069)\n",
      "tensor(-1.1828)\n",
      "tensor(-1.1591)\n",
      "tensor(-1.1360)\n",
      "tensor(-1.1132)\n",
      "tensor(-1.0910)\n",
      "tensor(-1.0691)\n",
      "tensor(-1.0478)\n",
      "tensor(-1.0268)\n",
      "tensor(-1.0063)\n",
      "tensor(-0.9861)\n",
      "tensor(-0.9664)\n",
      "tensor(-0.9471)\n",
      "tensor(-0.9282)\n",
      "tensor(-0.9096)\n",
      "tensor(-0.8914)\n",
      "tensor(-0.8736)\n",
      "tensor(-0.8561)\n",
      "tensor(-0.8390)\n",
      "tensor(-0.8222)\n",
      "tensor(-0.8058)\n",
      "tensor(-0.7896)\n",
      "tensor(-0.7738)\n",
      "tensor(-0.7584)\n",
      "tensor(-0.7432)\n",
      "tensor(-0.7283)\n",
      "tensor(-0.7138)\n",
      "tensor(-0.6995)\n",
      "tensor(-0.6855)\n",
      "tensor(-0.6718)\n",
      "tensor(-0.6584)\n",
      "tensor(-0.6452)\n",
      "tensor(-0.6323)\n",
      "tensor(-0.6196)\n",
      "tensor(-0.6073)\n",
      "tensor(-0.5951)\n",
      "tensor(-0.5832)\n",
      "tensor(-0.5715)\n",
      "tensor(-0.5601)\n",
      "tensor(-0.5489)\n",
      "tensor(-0.5379)\n",
      "tensor(-0.5272)\n",
      "tensor(-0.5166)\n",
      "tensor(-0.5063)\n",
      "tensor(-0.4962)\n",
      "tensor(-0.4862)\n",
      "tensor(-0.4765)\n",
      "tensor(-0.4670)\n",
      "tensor(-0.4577)\n",
      "tensor(-0.4485)\n",
      "tensor(-0.4395)\n",
      "tensor(-0.4307)\n",
      "tensor(-0.4221)\n",
      "tensor(-0.4137)\n",
      "tensor(-0.4054)\n",
      "tensor(-0.3973)\n",
      "tensor(-0.3894)\n",
      "tensor(-0.3816)\n",
      "tensor(-0.3739)\n",
      "tensor(-0.3665)\n",
      "tensor(-0.3591)\n",
      "tensor(-0.3519)\n",
      "tensor(-0.3449)\n",
      "tensor(-0.3380)\n",
      "tensor(-0.3312)\n",
      "tensor(-0.3246)\n",
      "tensor(-0.3181)\n",
      "tensor(-0.3118)\n",
      "tensor(-0.3055)\n",
      "tensor(-0.2994)\n",
      "tensor(-0.2934)\n",
      "tensor(-0.2876)\n",
      "tensor(-0.2818)\n",
      "tensor(-0.2762)\n",
      "tensor(-0.2707)\n",
      "tensor(-0.2652)\n",
      "tensor(-0.2599)\n",
      "tensor(-0.2547)\n",
      "tensor(-0.2496)\n",
      "tensor(-0.2446)\n",
      "tensor(-0.2398)\n",
      "tensor(-0.2350)\n",
      "tensor(-0.2303)\n",
      "tensor(-0.2257)\n",
      "tensor(-0.2211)\n",
      "tensor(-0.2167)\n",
      "tensor(-0.2124)\n",
      "tensor(-0.2081)\n",
      "tensor(-0.2040)\n",
      "tensor(-0.1999)\n",
      "tensor(-0.1959)\n",
      "tensor(-0.1920)\n",
      "tensor(-0.1881)\n",
      "tensor(-0.1844)\n",
      "tensor(-0.1807)\n",
      "tensor(-0.1771)\n",
      "tensor(-0.1735)\n",
      "tensor(-0.1701)\n",
      "tensor(-0.1667)\n",
      "tensor(-0.1633)\n",
      "tensor(-0.1601)\n",
      "tensor(-0.1569)\n",
      "tensor(-0.1537)\n",
      "tensor(-0.1506)\n",
      "tensor(-0.1476)\n",
      "tensor(-0.1447)\n",
      "tensor(-0.1418)\n",
      "tensor(-0.1390)\n",
      "tensor(-0.1362)\n",
      "tensor(-0.1335)\n",
      "tensor(-0.1308)\n",
      "tensor(-0.1282)\n",
      "tensor(-0.1256)\n",
      "tensor(-0.1231)\n",
      "tensor(-0.1206)\n",
      "tensor(-0.1182)\n",
      "tensor(-0.1159)\n",
      "tensor(-0.1135)\n",
      "tensor(-0.1113)\n",
      "tensor(-0.1090)\n",
      "tensor(-0.1069)\n",
      "tensor(-0.1047)\n",
      "tensor(-0.1026)\n",
      "tensor(-0.1006)\n",
      "tensor(-0.0986)\n",
      "tensor(-0.0966)\n",
      "tensor(-0.0947)\n",
      "tensor(-0.0928)\n",
      "tensor(-0.0909)\n",
      "tensor(-0.0891)\n",
      "tensor(-0.0873)\n",
      "tensor(-0.0856)\n",
      "tensor(-0.0839)\n",
      "tensor(-0.0822)\n",
      "tensor(-0.0805)\n",
      "tensor(-0.0789)\n",
      "tensor(-0.0773)\n",
      "tensor(-0.0758)\n",
      "tensor(-0.0743)\n",
      "tensor(-0.0728)\n",
      "tensor(-0.0713)\n",
      "tensor(-0.0699)\n",
      "tensor(-0.0685)\n",
      "tensor(-0.0671)\n",
      "tensor(-0.0658)\n",
      "tensor(-0.0645)\n",
      "tensor(-0.0632)\n",
      "tensor(-0.0619)\n",
      "tensor(-0.0607)\n",
      "tensor(-0.0595)\n",
      "tensor(-0.0583)\n",
      "tensor(-0.0571)\n",
      "tensor(-0.0560)\n",
      "tensor(-0.0549)\n",
      "tensor(-0.0538)\n",
      "tensor(-0.0527)\n",
      "tensor(-0.0516)\n",
      "tensor(-0.0506)\n",
      "tensor(-0.0496)\n",
      "tensor(-0.0486)\n",
      "tensor(-0.0476)\n",
      "tensor(-0.0467)\n",
      "tensor(-0.0457)\n",
      "tensor(-0.0448)\n",
      "tensor(-0.0439)\n",
      "tensor(-0.0431)\n",
      "tensor(-0.0422)\n",
      "tensor(-0.0413)\n",
      "tensor(-0.0405)\n",
      "tensor(-0.0397)\n",
      "tensor(-0.0389)\n",
      "tensor(-0.0381)\n",
      "tensor(-0.0374)\n",
      "tensor(-0.0366)\n",
      "tensor(-0.0359)\n",
      "tensor(-0.0352)\n",
      "tensor(-0.0345)\n",
      "tensor(-0.0338)\n",
      "tensor(-0.0331)\n",
      "tensor(-0.0324)\n",
      "tensor(-0.0318)\n",
      "tensor(-0.0312)\n",
      "tensor(-0.0305)\n",
      "tensor(-0.0299)\n",
      "tensor(-0.0293)\n",
      "tensor(-0.0287)\n",
      "tensor(-0.0282)\n",
      "tensor(-0.0276)\n",
      "tensor(-0.0271)\n",
      "tensor(-0.0265)\n",
      "tensor(-0.0260)\n",
      "tensor(-0.0255)\n",
      "tensor(-0.0250)\n",
      "tensor(-0.0245)\n",
      "tensor(-0.0240)\n",
      "tensor(-0.0235)\n",
      "tensor(-0.0230)\n",
      "tensor(-0.0226)\n",
      "tensor(-0.0221)\n",
      "tensor(-0.0217)\n",
      "tensor(-0.0212)\n",
      "tensor(-0.0208)\n",
      "tensor(-0.0204)\n",
      "tensor(-0.0200)\n",
      "tensor(-0.0196)\n",
      "tensor(-0.0192)\n",
      "tensor(-0.0188)\n",
      "tensor(-0.0184)\n",
      "tensor(-0.0181)\n",
      "tensor(-0.0177)\n",
      "tensor(-0.0173)\n",
      "tensor(-0.0170)\n",
      "tensor(-0.0167)\n",
      "tensor(-0.0163)\n",
      "tensor(-0.0160)\n",
      "tensor(-0.0157)\n",
      "tensor(-0.0154)\n",
      "tensor(-0.0151)\n",
      "tensor(-0.0148)\n",
      "tensor(-0.0145)\n",
      "tensor(-0.0142)\n",
      "tensor(-0.0139)\n",
      "tensor(-0.0136)\n",
      "tensor(-0.0133)\n",
      "tensor(-0.0131)\n",
      "tensor(-0.0128)\n",
      "tensor(-0.0126)\n",
      "tensor(-0.0123)\n",
      "tensor(-0.0121)\n",
      "tensor(-0.0118)\n",
      "tensor(-0.0116)\n",
      "tensor(-0.0113)\n",
      "tensor(-0.0111)\n",
      "tensor(-0.0109)\n",
      "tensor(-0.0107)\n",
      "tensor(-0.0105)\n",
      "tensor(-0.0103)\n",
      "tensor(-0.0101)\n",
      "tensor(-0.0099)\n",
      "tensor(-0.0097)\n",
      "tensor(-0.0095)\n",
      "tensor(-0.0093)\n",
      "tensor(-0.0091)\n",
      "tensor(-0.0089)\n",
      "tensor(-0.0087)\n",
      "tensor(-0.0086)\n",
      "tensor(-0.0084)\n",
      "tensor(-0.0082)\n",
      "tensor(-0.0080)\n",
      "tensor(-0.0079)\n",
      "tensor(-0.0077)\n",
      "tensor(-0.0076)\n",
      "tensor(-0.0074)\n",
      "tensor(-0.0073)\n",
      "tensor(-0.0071)\n",
      "tensor(-0.0070)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0067)\n",
      "tensor(-0.0066)\n",
      "tensor(-0.0064)\n",
      "tensor(-0.0063)\n",
      "tensor(-0.0062)\n",
      "tensor(-0.0061)\n",
      "tensor(-0.0059)\n",
      "tensor(-0.0058)\n",
      "tensor(-0.0057)\n",
      "tensor(-0.0056)\n",
      "tensor(-0.0055)\n",
      "tensor(-0.0054)\n",
      "tensor(-0.0053)\n",
      "tensor(-0.0052)\n",
      "tensor(-0.0051)\n",
      "tensor(-0.0050)\n",
      "tensor(-0.0049)\n",
      "tensor(-0.0048)\n",
      "tensor(-0.0047)\n",
      "tensor(-0.0046)\n",
      "tensor(-0.0045)\n",
      "tensor(-0.0044)\n",
      "tensor(-0.0043)\n",
      "tensor(-0.0042)\n",
      "tensor(-0.0041)\n",
      "tensor(-0.0040)\n",
      "tensor(-0.0040)\n",
      "tensor(-0.0039)\n",
      "tensor(-0.0038)\n",
      "tensor(-0.0037)\n",
      "tensor(-0.0037)\n",
      "tensor(-0.0036)\n",
      "tensor(-0.0035)\n",
      "tensor(-0.0034)\n",
      "tensor(-0.0034)\n",
      "tensor(-0.0033)\n",
      "tensor(-0.0032)\n",
      "tensor(-0.0032)\n",
      "tensor(-0.0031)\n",
      "tensor(-0.0031)\n",
      "tensor(-0.0030)\n",
      "tensor(-0.0029)\n",
      "tensor(-0.0029)\n",
      "tensor(-0.0028)\n",
      "tensor(-0.0028)\n",
      "tensor(-0.0027)\n",
      "tensor(-0.0026)\n",
      "tensor(-0.0026)\n",
      "tensor(-0.0025)\n",
      "tensor(-0.0025)\n",
      "tensor(-0.0024)\n",
      "tensor(-0.0024)\n",
      "tensor(-0.0023)\n",
      "tensor(-0.0023)\n",
      "tensor(-0.0023)\n",
      "tensor(-0.0022)\n",
      "tensor(-0.0022)\n",
      "tensor(-0.0021)\n",
      "tensor(-0.0021)\n",
      "tensor(-0.0020)\n",
      "tensor(-0.0020)\n",
      "tensor(-0.0020)\n",
      "tensor(-0.0019)\n",
      "tensor(-0.0019)\n",
      "tensor(-0.0018)\n",
      "tensor(-0.0018)\n",
      "tensor(-0.0018)\n",
      "tensor(-0.0017)\n",
      "tensor(-0.0017)\n",
      "tensor(-0.0017)\n",
      "tensor(-0.0016)\n",
      "tensor(-0.0016)\n",
      "tensor(-0.0016)\n",
      "tensor(-0.0015)\n",
      "tensor(-0.0015)\n",
      "tensor(-0.0015)\n",
      "tensor(-0.0014)\n",
      "tensor(-0.0014)\n",
      "tensor(-0.0014)\n",
      "tensor(-0.0014)\n",
      "tensor(-0.0013)\n",
      "tensor(-0.0013)\n",
      "tensor(-0.0013)\n",
      "tensor(-0.0013)\n",
      "tensor(-0.0012)\n",
      "tensor(-0.0012)\n",
      "tensor(-0.0012)\n",
      "tensor(-0.0012)\n",
      "tensor(-0.0011)\n",
      "tensor(-0.0011)\n",
      "tensor(-0.0011)\n",
      "tensor(-0.0011)\n",
      "tensor(-0.0010)\n",
      "tensor(-0.0010)\n",
      "tensor(-0.0010)\n",
      "tensor(-0.0010)\n",
      "tensor(-0.0010)\n",
      "tensor(-0.0009)\n",
      "tensor(-0.0009)\n",
      "tensor(-0.0009)\n",
      "tensor(-0.0009)\n",
      "tensor(-0.0009)\n",
      "tensor(-0.0009)\n",
      "tensor(-0.0008)\n",
      "tensor(-0.0008)\n",
      "tensor(-0.0008)\n",
      "tensor(-0.0008)\n",
      "tensor(-0.0008)\n",
      "tensor(-0.0008)\n",
      "tensor(-0.0007)\n",
      "tensor(-0.0007)\n",
      "tensor(-0.0007)\n",
      "tensor(-0.0007)\n",
      "tensor(-0.0007)\n",
      "tensor(-0.0007)\n",
      "tensor(-0.0007)\n",
      "tensor(-0.0006)\n",
      "tensor(-0.0006)\n",
      "tensor(-0.0006)\n",
      "tensor(-0.0006)\n",
      "tensor(-0.0006)\n",
      "tensor(-0.0006)\n",
      "tensor(-0.0006)\n",
      "tensor(-0.0006)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0004)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0002)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-0.0001)\n",
      "tensor(-9.8467e-05)\n",
      "tensor(-9.6440e-05)\n",
      "tensor(-9.4533e-05)\n",
      "tensor(-9.2626e-05)\n",
      "tensor(-9.0718e-05)\n",
      "tensor(-8.8930e-05)\n",
      "tensor(-8.7142e-05)\n",
      "tensor(-8.5354e-05)\n",
      "tensor(-8.3685e-05)\n",
      "y= tensor(0.) ; x= tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "利用自动微分和优化器求最小值\n",
    "\n",
    "作为对比， 如果不设置  optimizer.zero_grad()  # 梯度清零\n",
    "则梯度结果如下（明显是累加了）：\n",
    "tensor(-2.)       tensor(-2.)\n",
    "tensor(-3.9600)     tensor(-1.9600)\n",
    "tensor(-5.8408)     tensor(-1.9208)\n",
    "tensor(-7.6048)     tensor(-1.8824)\n",
    "tensor(-9.2167)     tensor(-1.8447)\n",
    "tensor(-10.6442)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c的最小值\n",
    "\n",
    "x = torch.tensor(0.0,requires_grad = True) # x需要被求导\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "optimizer = torch.optim.SGD(params=[x],lr = 0.01)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    result = a*torch.pow(x,2) + b*x + c \n",
    "    return(result)\n",
    "\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()  # 梯度清零\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    print(x.grad)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "print(\"y=\",f(x).data,\";\",\"x=\",x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
